#--------------------------- Title ---------------------------------------------
## Project: ML_KR_Project.Rproj
## Script:  H-O_ML_Random_Forests.R
## Programmer: Christopher Griffin
## Creation Date: 4/21/2024
## Edit Date:  4/21/2024
## Notes: This is the tutorial for random forests from Hands-on Machine
## Learning with R
#-------------------------------------------------------------------------------
#-------------------------------------------------------------------------------
# Chapter 11 Random Forests
#
# Random forests are a modification of bagged decision trees that build a large
# collection of de-correlated trees to further improve predictive performance.
# They have become a very popular “out-of-the-box” or “off-the-shelf” learning
# algorithm that enjoys good predictive performance with relatively little hyperparameter tuning.
#-------------------------------------------------------------------------------
#-------------------------------------------------------------------------------
# 11.1 Prerequisites
#
# This chapter leverages the following packages. Some of these packages play a
# supporting role; however, the emphasis is on how to implement random forests
# with the ranger (Wright and Ziegler 2017) and h2o packages.
#-------------------------------------------------------------------------------
# Helper packages
library(dplyr)    # for data wrangling
library(ggplot2)  # for awesome graphics
# install the following packages:
# install.packages("ranger")
# install.packages("h2o")
# install.packages("tidymodels")
# Modeling packages
library(ranger)   # a c++ implementation of random forest
library(h2o)      # a java-based implementation of random forest
library(tidymodels) # for the ames housing dataset.
#-------------------------------------------------------------------------------
# The basic algorithm for a regression or classification random forest can be
# generalized as follows:
#
#   1.  Given a training data set
#   2.  Select number of trees to build (n_trees)
#   3.  for i = 1 to n_trees do
#   4.  |  Generate a bootstrap sample of the original data
#   5.  |  Grow a regression/classification tree to the bootstrapped data
#   6.  |  for each split do
#   7.  |  | Select m_try variables at random from all p variables
#   8.  |  | Pick the best variable/split-point among the m_try
#   9.  |  | Split the node into two child nodes
#   10. |  end
#   11. | Use typical tree model stopping criteria to determine when a
#         tree is complete (but do not prune)
#   12. end
#   13. Output ensemble of trees
#-------------------------------------------------------------------------------
# We’ll continue working with the ames_train data set created in Section 2.7 to illustrate the main concepts.
# Stratified sampling with the rsample package
set.seed(123)
split <- initial_split(ames, prop = 0.7,
strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
# number of features
n_features <- length(setdiff(names(ames_train), "Sale_Price"))
# train a default random forest model
ames_rf1 <- ranger(
Sale_Price ~ .,
data = ames_train,
mtry = floor(n_features / 3),
respect.unordered.factors = "order",
seed = 123
)
# get OOB RMSE
(default_rmse <- sqrt(ames_rf1$prediction.error))
plot(ames_rf1)
# Get the number of features
p <- length(setdiff(names(ames_train), "Sale_Price"))
#create a sequence with intervals of 10
nt <- seq(1, p*10+1, 10)
# Create an empty vector to store OOB RMSE values
oob_rmse <- vector("numeric", length(nt))
for(i in 1:length(nt)){
rf <- ranger(Sale_Price ~ ., ames_train, num.trees = nt[i], write.forest = FALSE)
oob_mse[i] <- rf$prediction.error
}
# Get the number of features
p <- length(setdiff(names(ames_train), "Sale_Price"))
#create a sequence with intervals of 10
nt <- seq(1, p*10+1, 10)
# Create an empty vector to store OOB RMSE values
oob_rmse <- vector("numeric", length(nt))
for(i in 1:length(nt)){
rf <- ranger(Sale_Price ~ ., ames_train, num.trees = nt[i], write.forest = FALSE)
oob_rmse[i] <- rf$prediction.error
}
# create hyperparameter grid
hyper_grid <- expand.grid(
mtry = floor(n_features * c(.05, .15, .25, .333, .4)),
min.node.size = c(1, 3, 5, 10),
replace = c(TRUE, FALSE),
sample.fraction = c(.5, .63, .8),
rmse = NA
)
View(hyper_grid)
# execute full cartesian grid search
for(i in seq_len(nrow(hyper_grid))) {
# fit model for ith hyperparameter combination
fit <- ranger(
formula         = Sale_Price ~ .,
data            = ames_train,
num.trees       = n_features * 10,
mtry            = hyper_grid$mtry[i],
min.node.size   = hyper_grid$min.node.size[i],
replace         = hyper_grid$replace[i],
sample.fraction = hyper_grid$sample.fraction[i],
verbose         = FALSE,
seed            = 123,
respect.unordered.factors = 'order',
)
# export OOB error
hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}
View(hyper_grid)
View(hyper_grid)
n_features
# create hyperparameter grid
(n_features)^0.5
hyper_grid2 <- expand.grid(
mtry = floor(n_features * c(.05, .15, .25, .333, .4, (n_features)^0.5)),
min.node.size = c(1, 3, 5, 10),
replace = c(TRUE, FALSE),
sample.fraction = c(.5, .63, .8),
rmse = NA
)
View(hyper_grid2)
hyper_grid2 <- expand.grid(
mtry = floor(n_features * c(.05, .15, .25, .333, .4, (n_features)^(-0.5))),
min.node.size = c(1, 3, 5, 10),
replace = c(TRUE, FALSE),
sample.fraction = c(.5, .63, .8),
rmse = NA
)
View(hyper_grid2)
# create hyperparameter grid
1/(sqrt(n_features))
# create hyperparameter grid
1/(sqrt(n_features))
hyper_grid <- expand.grid(
mtry = floor(n_features * c(.05, .117, .15, .25, .333, .4)),
min.node.size = c(1, 3, 5, 10),
replace = c(TRUE, FALSE),
sample.fraction = c(.5, .63, .8),
rmse = NA
)
# execute full cartesian grid search
for(i in seq_len(nrow(hyper_grid))) {
# fit model for ith hyperparameter combination
fit <- ranger(
formula         = Sale_Price ~ .,
data            = ames_train,
num.trees       = n_features * 10,
mtry            = hyper_grid$mtry[i],
min.node.size   = hyper_grid$min.node.size[i],
replace         = hyper_grid$replace[i],
sample.fraction = hyper_grid$sample.fraction[i],
verbose         = FALSE,
seed            = 123,
respect.unordered.factors = 'order',
)
# export OOB error
hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}
# assess top 10 models
hyper_grid %>%
arrange(rmse) %>%
mutate(perc_gain = (default_rmse - rmse) / default_rmse * 100) %>%
head(10)
# convert training data to h2o object
train_h2o <- as.h2o(ames_train)
# To fit a random forest model with h2o, we first need to initiate our h2o session.
h2o.no_progress()
h2o.init(max_mem_size = "5g")
# To fit a random forest model with h2o, we first need to initiate our h2o session.
h2o.no_progress()
h2o.init(max_mem_size = "5g")
# Next, we need to convert our training and test data sets to objects that h2o can work with.
# convert training data to h2o object
train_h2o <- as.h2o(ames_train)
# set the response column to Sale_Price
response <- "Sale_Price"
# set the predictor names
predictors <- setdiff(colnames(ames_train), response)
# The following fits a default random forest model with h2o to illustrate that our baseline results (
#   OOB RMSE = 24439) are very similar to the baseline ranger model we fit earlier.
h2o_rf1 <- h2o.randomForest(
x = predictors,
y = response,
training_frame = train_h2o,
ntrees = n_features * 10,
seed = 123
)
h2o_rf1
# hyperparameter grid
hyper_grid <- list(
mtries = floor(n_features * c(.05, .15, .25, .333, .4)),
min_rows = c(1, 3, 5, 10),
max_depth = c(10, 20, 30),
sample_rate = c(.55, .632, .70, .80)
)
# random grid search strategy
search_criteria <- list(
strategy = "RandomDiscrete",
stopping_metric = "mse",
stopping_tolerance = 0.001,   # stop if improvement is < 0.1%
stopping_rounds = 10,         # over the last 10 models
max_runtime_secs = 60*5      # or stop search after 5 min.
)
# perform grid search
random_grid <- h2o.grid(
algorithm = "randomForest",
grid_id = "rf_random_grid",
x = predictors,
y = response,
training_frame = train_h2o,
hyper_params = hyper_grid,
ntrees = n_features * 10,
seed = 123,
stopping_metric = "RMSE",
stopping_rounds = 10,           # stop if last 10 trees added
stopping_tolerance = 0.005,     # don't improve RMSE by 0.5%
search_criteria = search_criteria
)
# collect the results and sort by our model performance metric
# of choice
random_grid_perf <- h2o.getGrid(
grid_id = "rf_random_grid",
sort_by = "mse",
decreasing = FALSE
)
random_grid_perf
